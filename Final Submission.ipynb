{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement 1:\n",
    "\n",
    "Create a data filtering pipeline that could remove low-quality text-video pairs from the dataset and thus improve video generation quality with less cost of model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Approach\n",
    "\n",
    "* Filter out low FPS text-video pairs, then regnerate metafile with only high FPS pairs.\n",
    "* Batch download youtube videos using updated metafile, split them using cut_videos_mlda.py\n",
    "* Recaption video clips.\n",
    "* Use CLIP as an evaluation metric to decide if a better dataset has been created which can improve video generation quality and reduce model generation cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of new metafile\n",
    "* We chose 30 FPS as the minimum frame rate for a high quality video.\n",
    "* This reduces the number of unique videos in the dataset from 18750 to 2773, thus reducing model training time by 6.76 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "filename_old = cwd + r'\\metafiles\\long_mlda_data.json'\n",
    "\n",
    "with open(filename_old, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter videos with fps >= 30\n",
    "filtered_videos = {}\n",
    "for video_id, video_info in data.items():\n",
    "    for clip_name, clip_info in video_info['clip'].items():\n",
    "        if clip_info['fps'] >= 30:\n",
    "            if video_id not in filtered_videos:\n",
    "                filtered_videos[video_id] = {}\n",
    "            filtered_videos[video_id][clip_name] = clip_info\n",
    "\n",
    "# Convert filtered_videos to JSON string\n",
    "filtered_json_data = json.dumps(filtered_videos, indent=2)\n",
    "\n",
    "# Print the filtered JSON data\n",
    "output_file_path = cwd + r'\\metafiles\\filtered_data.json'\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    output_file.write(filtered_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_path, 'r') as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "print(len(data2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Downloading of Videos\n",
    "\n",
    "* We created a list of youtube links from the metadata file, then proceeded to down a small number of videos using pytube.\n",
    "* We then split the videos using cut_videos_mlda_old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_list = []\n",
    "\n",
    "#converts json file into a list for easier comprehension\n",
    "for i in data.keys():\n",
    "    info_list.append(data[i])\n",
    "\n",
    "print(info_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of youtube links\n",
    "youtube_links_list = []\n",
    "youtube_id_list = []\n",
    "\n",
    "for i in range(len(info_list)):\n",
    "    youtube_links_list.append(info_list[i]['url'])\n",
    "\n",
    "\n",
    "for i in range(len(info_list)):\n",
    "    youtube_id_list.append(list(info_list[i]['clip'].keys())[0].split('.')[0])\n",
    "    \n",
    "print(youtube_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "number_of_videos = 10\n",
    "\n",
    "#skips any video downloading errors eg. age restricted errors\n",
    "for i in range(number_of_videos):\n",
    "    try:\n",
    "        print(f\"Downloading video {i+1}\")\n",
    "        YouTube(youtube_links_list[i]).streams.first().download('download_videos', filename=f\"{youtube_id_list[i]}.mp4\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our overall approach for generating captions\n",
    "\n",
    "* To generate captions, we will use the LLaVA (Large Language-and-Vision Assistant) end-to-end framework. The framework is used to train and deploy conversational agents. It combines LLMs with various input modalities such as text, images, and audio to enable multimodal interactions.\n",
    "\n",
    "* The LLaVA framework provides the Replica API, which serves as the interface for creating the conversational agents. The API provides methods to interact with the agent, such as sending messages, receiving replies, and managing conversations.\n",
    "\n",
    "* The pretrained LLaVA model that we used for our captioning combines a pre-trained CLIP ViT-L/14 visual encoder and large language model Vicuna with a simple projection matrix, and has been finetuned in a two stage procedure.\n",
    "\n",
    "* Since LLaVA is only able to generate captions from images, we have opted to take frames from each video clip and run it through LLaVa to generate the caption. **Due to our limited time, we are only taking 1 frame from each video, which we will assume to be representative of the entire video.**\n",
    "\n",
    "* Future improvements can include sampling mutiple frames in regular intervals for each clip, generating a caption for each frame, then taking advantage of LLaVa's context window to combine the contents of all clip frames to generate a more accurate video caption.\n",
    "\n",
    "Details for LLaVA model: https://llava-vl.github.io/\n",
    "\n",
    "Replica API implementation: https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaVa Requirements and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install replicate\n",
    "!pip install \"pyautogen[lmm]>=0.2.3\"\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from PIL import Image\n",
    "from termcolor import colored\n",
    "\n",
    "import autogen\n",
    "from autogen import Agent, AssistantAgent, ConversableAgent, UserProxyAgent\n",
    "from autogen.agentchat.contrib.llava_agent import LLaVAAgent, llava_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Replicate API key must be generated to use Replicate. This can be done by making an account at https://replicate.com/account/api-tokens and generating an API token on the website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set api key for REPLICATE api\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_RK3RENh7f8CY4elCd9CklVenGckfo4k39Ylef\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaVa Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LLaVa can be used either locally or remotely.\n",
    "* We have chosen a remote setup using Replicate to host LLaVa, as we faced many compatibility issues with the local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAVA_MODE = \"remote\"\n",
    "assert LLAVA_MODE in [\"local\", \"remote\"]\n",
    "\n",
    "if LLAVA_MODE == \"remote\":\n",
    "    import replicate\n",
    "\n",
    "    llava_config_list = [\n",
    "        {\n",
    "            \"model\": \"-\",  # The model name doesn't matter here right now.\n",
    "            \"api_key\": \"None\",  # already setup using os.environ[\"REPLICATE_API_TOKEN\"]\n",
    "            \"base_url\": \"yorickvp/llava-13b:2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591\",\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Images from Video Clips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our video clips are stored in a folder in our current working directory. The folder name may change from time to time, thus we have chosen to iterate through all mp4 files in the current working directory to get the clip names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_from_video_directory = os.getcwd()\n",
    "filenames = []\n",
    "\n",
    "def get_mp4_files_recursive(folder_path):\n",
    "    mp4_files = []\n",
    "    # Walk through the directory tree\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                mp4_files.append(os.path.join(root, file))\n",
    "    return mp4_files\n",
    "\n",
    "filenames = get_mp4_files_recursive(images_from_video_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code block below details how each frame is generated for each video clip.\n",
    "* First, FFMPEG reads each input video and generates an output frame for that video, which is stored in the folder \"test_images\" in the current working directory.\n",
    "* The first second of each video clip is ignored as that period may represent a transition between different scenes which will generate an image with irrelevant information (eg. the transition animation) that will invalidate the caption generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates images in the folder test_images\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"test_images\"\n",
    "abs_path_list = []\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Loop through the filenames\n",
    "for file in filenames:\n",
    "    output_filename = os.path.join(folder_path, os.path.basename(file) + \"_output.png\")\n",
    "    abs_path_list.append(rf'{os.path.abspath(output_filename)}')\n",
    "    #frame generated is after first second\n",
    "    !ffmpeg -ss 00:00:01 -i \"$file\" -vf fps=1 \"$output_filename\"\n",
    "    print(abs_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing Captions using Replicate and Storage of Image-Caption pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To generate captions, we need to prompt LLaVA and receive a caption in response.\n",
    "* However before this, we have to first initialise instances of a user (user_proxy) and Llava (image_agent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_agent = LLaVAAgent(\n",
    "    name=\"image-explainer\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    llm_config={\"config_list\": llava_config_list, \"temperature\": 0.5, \"max_new_tokens\": 1000},\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"groupchat\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    human_input_mode=\"NEVER\",  # Try between ALWAYS or NEVER\n",
    "    max_consecutive_auto_reply=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, prompts can be sent to the LLaVa agent. However, the image may not be encoded in a way that can be read by LLaVa, thus we process the image to encode it in base64. A try except loop is used to skip across any conversion failures.\n",
    "* At the same time, we want to store the generated captions in sequence with the image files. This is done by calling the last_message method of the image_agent(representing the LLaVAAgent class) to store the generated caption as the obtained_caption variable.\n",
    "* Finally, image file paths and the captions are stored as key value pairs in a dictionary for futher processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "#storage of text-image pairs\n",
    "\n",
    "image_text_pairs = {}\n",
    "\n",
    "for abs_path in abs_path_list:\n",
    "    image_path = abs_path\n",
    "    try:\n",
    "        # Read the image as binary data\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            img_data = img_file.read()\n",
    "\n",
    "        # Encode the binary image data to base64\n",
    "        encoded_image = base64.b64encode(img_data).decode(\"utf-8\")\n",
    "\n",
    "        # Ask the question with an image\n",
    "        user_proxy.initiate_chat(\n",
    "            image_agent,\n",
    "            message=\"\"\"Caption this image as accurately as possible. Include all details, such as the environment, objects, what may happen in the past or near future, the predicted and current actions happening in the scene. Summarise the description in as much detail as possible within 40 words. sets of keywords in commas can be used to shorten the description instead of outputing full sentences\n",
    "        <img {image_path}>.\"\"\".format(image_path=image_path),\n",
    "        )\n",
    "\n",
    "        obtained_caption = image_agent.last_message()['content'][0]['text']\n",
    "        \n",
    "\n",
    "        #caption list obtained\n",
    "        image_text_pairs[abs_path] = [obtained_caption]\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Prompt Statement\n",
    "\n",
    "Our prompt statement is as follows:\n",
    "\n",
    "**Caption this image as accurately as possible. Include all details, such as the environment, objects, what may happen in the past or near future, the predicted and current actions happening in the scene. Summarise the description in as much detail as possible within 40 words. sets of keywords in commas can be used to shorten the description instead of outputing full sentences.**\n",
    "\n",
    "The prompt highlights major details such as the environment and events in the past and future to ensure that the caption encompasses these important aspects. Keywords are used to streamline the generated caption, allowing detailed yet short captions to keep within the context length of CLIP. A summary of 40 words is required at the end to prevent errors due to exceeding the context length of CLIP when an extremely detailed caption is input into CLIP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Score Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that we have obtained the image file paths and captions, we will be able to compute the clip score for each image using the openai-clip package.\n",
    "\n",
    "* Reference: https://unimatrixz.com/blog/latent-space-clip-score/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Requirements and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from clip import *\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The get_clip_score function below loads a pre-trained CLIP model and an image, preprocesses the image, tokenizes the input text, generates embeddings for both the image and text, normalizes the features, calculates the cosine similarity between them, and returns the CLIP score as a measure of similarity between the image and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_score(image_path, text):\n",
    "    # Load the pre-trained CLIP model and the image\n",
    "\n",
    "    model, preprocess = clip.load('ViT-B/32')\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Preprocess the image and tokenize the text\n",
    "    image_input = preprocess(image).unsqueeze(0)\n",
    "    text_input = clip.tokenize([text])\n",
    "    \n",
    "    # Move the inputs to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_input = image_input.to(device)\n",
    "    text_input = text_input.to(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Generate embeddings for the image and text\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_input)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the cosine similarity to get the CLIP score\n",
    "    clip_score = torch.matmul(image_features, text_features.T).item()\n",
    "    \n",
    "    return clip_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code block below runs gets the clip score for every image-text pair and modifies the image_text_pairs dictionary accordingly.\n",
    "* This creates the resulting format for image_text_pairs: {image_file_path1:[obtained_caption,clip_score]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_image_path,caption in image_text_pairs.items():\n",
    "    image_text_pairs[raw_image_path].append(get_clip_score(raw_image_path,caption[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code block below calculates the average clip score of all frames/videos.\n",
    "* Based on the provided reference, our custom-prompted LLaVA model achieves the 13th rank on the leaderboard.\n",
    "\n",
    "Image Classification in the Wild leaderboard: https://eval.ai/web/challenges/challenge-page/1832/leaderboard/4298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for key,value in image_text_pairs.items():\n",
    "    print(value[1])\n",
    "    counter+=value[1]\n",
    "\n",
    "print(f'Average clip score is:{counter/len(image_text_pairs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, the clip scores can be sorted accordingly to return the best text to video pairs.\n",
    "* Some processing is done before sorting in the next section to recover the original file names of the videos so that they can be retrieved to be fed into the video generation model as stated in the problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_list = {}\n",
    "\n",
    "for key,value in image_text_pairs.items():\n",
    "    selection_list_key = key[:key.rfind(\"_\")]\n",
    "    selection_list_key = selection_list_key.rsplit(\"\\\\\", 1)[1]\n",
    "    selection_list[selection_list_key] = value\n",
    "\n",
    "print(selection_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting list of best video-text pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary items based on the numerical score in descending order\n",
    "sorted_selection = sorted(selection_list.items(), key=lambda x: x[1][1], reverse=True)\n",
    "\n",
    "print(sorted_selection)\n",
    "# Extract the keys and values of the top two items\n",
    "\n",
    "top_files = [(item[0], item[1][0]) for item in sorted_selection[:10000]]\n",
    "\n",
    "# Append the names and values of the top two files to my_list\n",
    "my_list = []\n",
    "my_list.extend(top_files)\n",
    "\n",
    "# print(my_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have managed to reduce size of the original dataset by 6.76 times by filtering out low FPS pairs.\n",
    "* Our regenerated captions have an average clip score of about 0.318 which is better that quite a few models.\n",
    "* Thus we can conclude that our model can improve video generation quality with less cost of model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
